# -*- coding: utf-8 -*-
"""
Script para treinamento de um modelo de reconhecimento de sinais (Libras)
utilizando MediaPipe para extração de pontos-chave e uma rede neural LSTM
para classificação de sequências de vídeo.
"""

# --- 1. Importação de Bibliotecas ---
import os
from typing import List, Tuple, Dict, Any, Optional

import cv2
import numpy as np
import mediapipe as mp
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical

# --- 2. Configuração Global ---

# Configurações do MediaPipe
mp_holistic = mp.solutions.holistic
mp_drawing = mp.solutions.drawing_utils

# Configurações do Dataset e Pré-processamento
DATA_PATH = os.path.join('samples')
ACTIONS = np.array(['obrigado', 'nada'])
MAX_FRAMES = 30  # Número de frames por sequência de vídeo

# Configurações de Treinamento
TEST_SPLIT_SIZE = 0.2
VALIDATION_SPLIT_SIZE = 0.2
EPOCHS = 100
BATCH_SIZE = 32
RANDOM_STATE = 42

# --- 3. Funções de Processamento de Dados ---

def mediapipe_detection(image: np.ndarray, model: mp_holistic.Holistic) -> Tuple[np.ndarray, Any]:
    """
    Processa uma imagem com o modelo MediaPipe Holistic para detectar pontos-chave.

    Args:
        image (np.ndarray): O frame do vídeo (em formato BGR).
        model (mp_holistic.Holistic): A instância do modelo MediaPipe Holistic.

    Returns:
        Tuple[np.ndarray, Any]: Uma tupla contendo a imagem original e os resultados da detecção.
    """
    # Converte a cor da imagem de BGR para RGB para processamento pelo MediaPipe
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image_rgb.flags.writeable = False
    
    # Realiza a detecção
    results = model.process(image_rgb)
    
    # Retorna a imagem ao formato original
    image_rgb.flags.writeable = True
    image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)
    
    return image_bgr, results

def extract_keypoints(results: Any) -> np.ndarray:
    """
    Extrai e concatena os pontos-chave de pose, mão esquerda e mão direita dos resultados do MediaPipe.

    Args:
        results (Any): O objeto de resultados retornado pelo MediaPipe.

    Returns:
        np.ndarray: Um array NumPy achatado contendo todos os pontos-chave.
    """
    # Extrai pontos-chave da pose, preenchendo com zeros se não houver detecção
    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() \
        if results.pose_landmarks else np.zeros(33 * 4)
        
    # Extrai pontos-chave da mão esquerda
    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() \
        if results.left_hand_landmarks else np.zeros(21 * 3)
        
    # Extrai pontos-chave da mão direita
    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() \
        if results.right_hand_landmarks else np.zeros(21 * 3)
        
    return np.concatenate([pose, lh, rh])

def load_sequences_and_labels(data_path: str, actions: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """
    Carrega os dados de vídeo, extrai pontos-chave e os organiza em sequências e rótulos.

    Args:
        data_path (str): O caminho para o diretório raiz do dataset.
        actions (np.ndarray): Um array com os nomes das classes (ações).

    Returns:
        Tuple[np.ndarray, np.ndarray]: Uma tupla contendo o array de sequências (X) e o array de rótulos (y) em formato one-hot.
    """
    label_map = {label: num for num, label in enumerate(actions)}
    sequences, labels = [], []

    print("Iniciando carregamento e processamento dos dados...")
    for action in actions:
        action_path = os.path.join(data_path, action)
        if not os.path.isdir(action_path):
            print(f"Aviso: Diretório não encontrado para a ação '{action}'. Pulando.")
            continue

        video_files = [f for f in os.listdir(action_path) if f.lower().endswith(('.mp4', '.avi', '.mov'))]
        print(f"Processando {len(video_files)} vídeos para a ação '{action}'...")

        for video_file in video_files:
            video_path = os.path.join(action_path, video_file)
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                print(f"Erro ao abrir o arquivo de vídeo: {video_path}")
                continue

            frames = []
            with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:
                while cap.isOpened():
                    ret, frame = cap.read()
                    if not ret:
                        break
                    _, results = mediapipe_detection(frame, holistic)
                    keypoints = extract_keypoints(results)
                    frames.append(keypoints)
            cap.release()

            # Normaliza a sequência para ter um comprimento fixo (MAX_FRAMES)
            if frames:
                if len(frames) > MAX_FRAMES:
                    # Se o vídeo for longo, seleciona frames uniformemente espaçados
                    indices = np.linspace(0, len(frames) - 1, MAX_FRAMES, dtype=int)
                    frames = [frames[i] for i in indices]
                elif len(frames) < MAX_FRAMES:
                    # Se o vídeo for curto, preenche com zeros no final (padding)
                    padding = [np.zeros(frames[0].shape) for _ in range(MAX_FRAMES - len(frames))]
                    frames.extend(padding)
                
                sequences.append(frames)
                labels.append(label_map[action])

    print("\nResumo do Carregamento:")
    for action in actions:
        count = sum(1 for label in labels if label == label_map[action])
        print(f"  - Ação '{action}': {count} sequências carregadas.")
    
    return np.array(sequences), to_categorical(labels).astype(int)

# --- 4. Funções de Modelo e Treinamento ---

def build_lstm_model(input_shape: Tuple[int, int], num_classes: int) -> Sequential:
    """
    Constrói, compila e retorna o modelo LSTM.

    Args:
        input_shape (Tuple[int, int]): A forma dos dados de entrada (frames, features).
        num_classes (int): O número de classes de saída.

    Returns:
        Sequential: O modelo Keras compilado.
    """
    model = Sequential([
        LSTM(64, return_sequences=True, activation='relu', input_shape=input_shape),
        Dropout(0.2),
        LSTM(128, return_sequences=True, activation='relu'),
        Dropout(0.2),
        LSTM(64, return_sequences=False, activation='relu'),
        Dropout(0.2),
        Dense(64, activation='relu'),
        Dropout(0.3),
        Dense(32, activation='relu'),
        Dense(num_classes, activation='softmax')
    ])
    
    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])
    model.summary()
    return model

def train_model(model: Sequential, X_train: np.ndarray, y_train: np.ndarray, 
                X_val: np.ndarray, y_val: np.ndarray, 
                class_weights: Optional[Dict[int, float]] = None) -> tf.keras.callbacks.History:
    """
    Treina o modelo com os dados fornecidos e callbacks.

    Args:
        model (Sequential): O modelo a ser treinado.
        X_train, y_train (np.ndarray): Dados de treinamento.
        X_val, y_val (np.ndarray): Dados de validação.
        class_weights (Optional[Dict[int, float]]): Pesos para lidar com classes desbalanceadas.

    Returns:
        tf.keras.callbacks.History: O histórico de treinamento.
    """
    # Callbacks para otimizar o treinamento
    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.0001, verbose=1)
    
    print("\nIniciando o treinamento do modelo...")
    history = model.fit(
        X_train, y_train,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        validation_data=(X_val, y_val),
        class_weight=class_weights,
        callbacks=[early_stopping, reduce_lr],
        verbose=1
    )
    return history

# --- 5. Funções de Avaliação e Conversão ---

def evaluate_model(model: Sequential, X_test: np.ndarray, y_test: np.ndarray, actions: np.ndarray):
    """
    Avalia o desempenho do modelo no conjunto de teste.

    Args:
        model (Sequential): O modelo treinado.
        X_test, y_test (np.ndarray): Dados de teste.
        actions (np.ndarray): Nomes das classes para exibição.
    """
    print("\n" + "="*20 + " AVALIAÇÃO DO MODELO " + "="*20)
    predictions = model.predict(X_test, verbose=0)
    predicted_classes = np.argmax(predictions, axis=1)
    true_classes = np.argmax(y_test, axis=1)

    # Acurácia geral
    accuracy = np.mean(predicted_classes == true_classes)
    print(f"Acurácia Geral no Conjunto de Teste: {accuracy:.2%}\n")

    # Acurácia por classe
    print("Acurácia por Classe:")
    for i, action in enumerate(actions):
        class_mask = (true_classes == i)
        if np.sum(class_mask) > 0:
            class_accuracy = np.mean(predicted_classes[class_mask] == true_classes[class_mask])
            print(f"  - {action}: {class_accuracy:.2%} ({np.sum(class_mask)} amostras)")

def convert_and_save_tflite(model: Sequential, filename: str = 'asl_model.tflite'):
    """
    Converte o modelo Keras para o formato TensorFlow Lite e o salva.

    Args:
        model (Sequential): O modelo Keras treinado.
        filename (str): O nome do arquivo para o modelo TFLite.
    """
    print(f"\nConvertendo o modelo para TensorFlow Lite...")
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    
    # Configurações para compatibilidade e otimização
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
    converter._experimental_lower_tensor_list_ops = False
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    
    tflite_model = converter.convert()
    
    with open(filename, 'wb') as f:
        f.write(tflite_model)
    print(f"Modelo TFLite salvo com sucesso como '{filename}'")

# --- 6. Fluxo de Execução Principal ---

def main():
    """
    Função principal que orquestra todo o processo de carregamento, treinamento e avaliação.
    """
    # Etapa 1: Carregar e pré-processar os dados
    X, y = load_sequences_and_labels(DATA_PATH, ACTIONS)

    if X.shape[0] == 0:
        print("\nNenhum dado foi carregado. Verifique a estrutura de pastas e os arquivos de vídeo.")
        return

    # Etapa 2: Dividir os dados em conjuntos de treino, validação e teste
    X_train_val, X_test, y_train_val, y_test = train_test_split(
        X, y, test_size=TEST_SPLIT_SIZE, random_state=RANDOM_STATE, stratify=y
    )
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_val, y_train_val, test_size=VALIDATION_SPLIT_SIZE, random_state=RANDOM_STATE, stratify=y_train_val
    )
    print(f"\nDivisão dos dados concluída:")
    print(f"  - Treinamento: {len(X_train)} amostras")
    print(f"  - Validação:   {len(X_val)} amostras")
    print(f"  - Teste:       {len(X_test)} amostras")

    # Etapa 3: Calcular pesos de classe para dados desbalanceados
    y_train_classes = np.argmax(y_train, axis=1)
    class_weights_array = compute_class_weight('balanced', classes=np.unique(y_train_classes), y=y_train_classes)
    class_weights = {i: weight for i, weight in enumerate(class_weights_array)}
    print(f"Pesos de classe calculados: {class_weights}")

    # Etapa 4: Construir e treinar o modelo
    input_shape = (X_train.shape[1], X_train.shape[2])
    num_classes = y_train.shape[1]
    
    model = build_lstm_model(input_shape, num_classes)
    train_model(model, X_train, y_train, X_val, y_val, class_weights)

    # Etapa 5: Avaliar o modelo treinado
    evaluate_model(model, X_test, y_test, ACTIONS)

    # Etapa 6: Salvar os modelos
    print("\nSalvando os artefatos do modelo...")
    model.save('asl_model.h5')
    print("Modelo Keras salvo como 'asl_model.h5'")
    
    convert_and_save_tflite(model)
    
    print("\n--- Processo de Treinamento Concluído com Sucesso ---")

if __name__ == '__main__':
    main()
